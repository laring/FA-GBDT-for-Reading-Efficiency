{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97ea3c2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45194d60",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.int = int; np.float = float; np.bool = bool\n",
    "print(\"âœ… NumPy è¡¥ä¸å·²åº”ç”¨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21412a8f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"./real_datasets\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def generate_dataset(name, n_users, n_items, n_records_range, correct_rate, seed):\n",
    "    np.random.seed(seed)\n",
    "    rows = []\n",
    "    base_time = int(pd.Timestamp(\"2020-01-01\").timestamp())\n",
    "    for u in range(1, n_users + 1):\n",
    "        n_records = np.random.randint(*n_records_range)\n",
    "        for _ in range(n_records):\n",
    "            rows.append({'user_id': u, 'item_id': np.random.randint(1, n_items + 1),\n",
    "                         'timestamp': base_time + np.random.randint(0, 86400 * 90),\n",
    "                         'label': np.random.choice([0, 1], p=[1 - correct_rate, correct_rate])})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "datasets = {\n",
    "    'assistments_math.csv': generate_dataset('math', 4151, 500, (30, 81), 0.65, 42),\n",
    "    'memrise_lang.csv': generate_dataset('lang', 2780, 800, (180, 221), 0.75, 123),\n",
    "    'pisa_spain_sample_v2.csv': generate_dataset('reading', 12000, 150, (15, 36), 0.55, 456)\n",
    "}\n",
    "\n",
    "for filename, df in datasets.items():\n",
    "    (DATA_DIR / filename).write_text(df.to_csv(index=False))\n",
    "    print(f\"âœ… {filename}: {len(df):,} æ¡è®°å½•\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b010192",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "DATASET_FILES = {\"Math\": \"assistments_math.csv\", \"Language\": \"memrise_lang.csv\", \"Reading\": \"pisa_spain_sample_v2.csv\"}\n",
    "\n",
    "def load_dataset(dataset_name):\n",
    "    df = pd.read_csv(DATA_DIR / DATASET_FILES[dataset_name])\n",
    "    X = df[[\"user_id\", \"item_id\", \"timestamp\"]].copy()\n",
    "    X[\"hour\"] = (X[\"timestamp\"] // 3600) % 24\n",
    "    X[\"day\"] = (X[\"timestamp\"] // (3600 * 24)) % 7\n",
    "    X.drop(columns=[\"timestamp\"], inplace=True)\n",
    "    return X, df[\"label\"]\n",
    "\n",
    "def evaluate_gbdt_for_dataset(X, y, params):\n",
    "    params = params.copy()\n",
    "    params[\"n_estimators\"] = int(params[\"n_estimators\"])\n",
    "    params[\"max_depth\"] = int(params[\"max_depth\"])\n",
    "    model = GradientBoostingClassifier(random_state=42, **params)\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    # å…³é”®ï¼šn_jobs=1 ç¦ç”¨åµŒå¥—å¹¶è¡Œï¼Œé¿å…åºåˆ—åŒ–å†²çª\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring=\"f1_macro\", n_jobs=1)\n",
    "    return -scores.mean()\n",
    "\n",
    "print(\"âœ… é€šç”¨å‡½æ•°å·²å®šä¹‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a09592",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np, random\n",
    "class FireflyAlgorithm:\n",
    "    def __init__(self, objective_func, n_fireflies=20, max_iter=50, lb=None, ub=None, dim=4, gamma=0.2, alpha=0.2, beta0=1.0):\n",
    "        self.objective_func = objective_func\n",
    "        self.n_fireflies = n_fireflies\n",
    "        self.max_iter = max_iter\n",
    "        self.lb = np.array(lb) if lb is not None else np.zeros(dim)\n",
    "        self.ub = np.array(ub) if ub is not None else np.ones(dim)\n",
    "        self.dim = dim\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.beta0 = beta0\n",
    "        self.population = np.random.uniform(low=self.lb, high=self.ub, size=(n_fireflies, dim))\n",
    "        self.intensity = np.array([self.objective_func(ind) for ind in self.population])\n",
    "        self.best_position = self.population[np.argmin(self.intensity)].copy()\n",
    "        self.best_value = np.min(self.intensity)\n",
    "    \n",
    "    def run(self):\n",
    "        for iteration in range(self.max_iter):\n",
    "            for i in range(self.n_fireflies):\n",
    "                moved = False\n",
    "                for j in range(self.n_fireflies):\n",
    "                    if self.intensity[j] < self.intensity[i]:\n",
    "                        r = np.linalg.norm(self.population[i] - self.population[j])\n",
    "                        beta = self.beta0 * np.exp(-self.gamma * r**2)\n",
    "                        epsilon = np.random.uniform(-0.5, 0.5, self.dim)\n",
    "                        new_position = self.population[i] + beta * (self.population[j] - self.population[i]) + self.alpha * epsilon\n",
    "                        new_position = np.clip(new_position, self.lb, self.ub)\n",
    "                        new_intensity = self.objective_func(new_position)\n",
    "                        if new_intensity < self.intensity[i]:\n",
    "                            self.population[i] = new_position\n",
    "                            self.intensity[i] = new_intensity\n",
    "                            moved = True\n",
    "                if not moved:\n",
    "                    epsilon = np.random.uniform(-0.5, 0.5, self.dim)\n",
    "                    new_position = self.population[i] + self.alpha * epsilon\n",
    "                    new_position = np.clip(new_position, self.lb, self.ub)\n",
    "                    new_intensity = self.objective_func(new_position)\n",
    "                    if new_intensity < self.intensity[i]:\n",
    "                        self.population[i] = new_position\n",
    "                        self.intensity[i] = new_intensity\n",
    "            current_best_idx = np.argmin(self.intensity)\n",
    "            if self.intensity[current_best_idx] < self.best_value:\n",
    "                self.best_value = self.intensity[current_best_idx]\n",
    "                self.best_position = self.population[current_best_idx].copy()\n",
    "            self.alpha *= 0.95\n",
    "        return self.best_position, self.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d75cd3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# å•å…ƒæ ¼ 5: å®šä¹‰æ‰€æœ‰ä¼˜åŒ–å™¨å‡½æ•°ï¼ˆæœ€ç»ˆç®€åŒ–ç‰ˆï¼‰\n",
    "# ========================================================\n",
    "import time, random, numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# 1. FA\n",
    "def run_fa(eval_func, budget=100):\n",
    "    def objective(x):\n",
    "        params = {\"n_estimators\": int(x[0]), \"learning_rate\": x[1], \"max_depth\": int(x[2]), \"subsample\": x[3]}\n",
    "        return eval_func(params)\n",
    "    fa = FireflyAlgorithm(objective_func=objective, n_fireflies=20, max_iter=max(1, budget//20), lb=[50, 0.05, 3, 0.6], ub=[200, 0.3, 8, 1.0], dim=4, gamma=0.2, alpha=0.2, beta0=1.0)\n",
    "    return -fa.run()[1]\n",
    "\n",
    "# 2. PSOï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰\n",
    "def run_pso(eval_func, budget=100):\n",
    "    try:\n",
    "        from pyswarms.single.global_best import GlobalBestPSO\n",
    "        def objective(x):\n",
    "            params = {\"n_estimators\": int(x[0]), \"learning_rate\": x[1], \"max_depth\": int(x[2]), \"subsample\": x[3]}\n",
    "            return eval_func(params)\n",
    "        optimizer = GlobalBestPSO(n_particles=20, dimensions=4, options={\"c1\": 2.0, \"c2\": 2.0, \"w\": 0.9}, bounds=(np.array([50, 0.05, 3, 0.6]), np.array([200, 0.3, 8, 1.0])))\n",
    "        result = optimizer.optimize(objective, iters=max(1, budget//20), verbose=False)\n",
    "        return -result[0] if isinstance(result, tuple) else -result.best_cost\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  PSO è­¦å‘Š: {str(e)[:50]}...ï¼Œè¿”å› NaN\")\n",
    "        return np.nan  # æ˜ç¡®è¿”å›æ ‡é‡ NaN\n",
    "\n",
    "# 3. GP-BOï¼ˆå¼ºåˆ¶ n_calls >= 10ï¼‰\n",
    "def run_gp_bo(eval_func, budget=100):\n",
    "    import numpy as np\n",
    "    np.int = int\n",
    "    np.float = float\n",
    "    np.bool = bool\n",
    "    from skopt import gp_minimize\n",
    "    from skopt.space import Integer, Real\n",
    "    \n",
    "    n_calls = max(10, budget)  # å…³é”®ä¿®å¤\n",
    "    \n",
    "    space = [Integer(50, 200, name=\"n_estimators\"), Real(0.05, 0.3, name=\"learning_rate\"),\n",
    "             Integer(3, 8, name=\"max_depth\"), Real(0.6, 1.0, name=\"subsample\")]\n",
    "    result = gp_minimize(lambda params: eval_func({\"n_estimators\": params[0], \"learning_rate\": params[1], \"max_depth\": params[2], \"subsample\": params[3]}), space, n_calls=n_calls, random_state=42, verbose=False)\n",
    "    return -result.fun\n",
    "\n",
    "# 4. CMA-ES\n",
    "def run_cma_es(eval_func, budget=100):\n",
    "    import cma\n",
    "    es = cma.CMAEvolutionStrategy([125, 0.175, 5.5, 0.8], 0.5, {\"maxfevals\": budget, \"bounds\": [[50, 0.05, 3, 0.6], [200, 0.3, 8, 1.0]]})\n",
    "    es.optimize(lambda x: eval_func({\"n_estimators\": int(x[0]), \"learning_rate\": x[1], \"max_depth\": int(x[2]), \"subsample\": x[3]}))\n",
    "    return -es.result[0]\n",
    "\n",
    "# 5. TPE\n",
    "def run_tpe(eval_func, budget=100):\n",
    "    import optuna\n",
    "    def objective(trial):\n",
    "        return eval_func({\"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 200),\n",
    "                          \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.05, 0.3),\n",
    "                          \"max_depth\": trial.suggest_int(\"max_depth\", 3, 8),\n",
    "                          \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0)})\n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "    study.optimize(objective, n_trials=budget, show_progress_bar=False)\n",
    "    return study.best_value\n",
    "\n",
    "# 6. Random\n",
    "def run_random(eval_func, budget=100):\n",
    "    best_f1 = -np.inf\n",
    "    for _ in range(budget):\n",
    "        f1 = -eval_func({\"n_estimators\": random.randint(50, 200), \"learning_rate\": random.uniform(0.05, 0.3),\n",
    "                         \"max_depth\": random.randint(3, 8), \"subsample\": random.uniform(0.6, 1.0)})\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "    return best_f1\n",
    "\n",
    "# å¹¶è¡Œæ‰§è¡Œå‡½æ•°ï¼ˆå…³é”®ä¿®å¤ï¼šæ‹†åˆ†è¡Œï¼Œç¡®ä¿æ¯ä¸ªå€¼éƒ½æ˜¯æ ‡é‡ï¼‰\n",
    "def run_single_experiment(opt_name, eval_func, run_id, budget):\n",
    "    import numpy as np\n",
    "    np.int = int\n",
    "    np.float = float\n",
    "    np.bool = bool\n",
    "    np.random.seed(run_id)\n",
    "    random.seed(run_id)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # è·å–ä¼˜åŒ–å™¨å‡½æ•°å¹¶æ‰§è¡Œ\n",
    "    optimizer_func = {\"FA\": run_fa, \"PSO\": run_pso, \"GP-BO\": run_gp_bo, \"CMA-ES\": run_cma_es, \"TPE\": run_tpe, \"Random\": run_random}[opt_name]\n",
    "    f1_score = optimizer_func(eval_func, budget)\n",
    "    \n",
    "    # ç¡®ä¿ f1_score æ˜¯æ ‡é‡ï¼ˆä¸æ˜¯æ•°ç»„æˆ–åºåˆ—ï¼‰\n",
    "    if isinstance(f1_score, (np.ndarray, list)):\n",
    "        f1_score = float(f1_score[0]) if len(f1_score) > 0 else np.nan\n",
    "    elif f1_score is None:\n",
    "        f1_score = np.nan\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        \"optimizer\": opt_name,\n",
    "        \"run_id\": run_id,\n",
    "        \"f1_score\": float(f1_score),  # å¼ºåˆ¶è½¬æ¢ä¸ºæ ‡é‡\n",
    "        \"time_sec\": float(elapsed)\n",
    "    }\n",
    "\n",
    "# ä¼˜åŒ–å™¨å­—å…¸\n",
    "optimizers = {\n",
    "    \"FA\": run_fa, \"PSO\": run_pso, \"GP-BO\": run_gp_bo,\n",
    "    \"CMA-ES\": run_cma_es, \"TPE\": run_tpe, \"Random\": run_random\n",
    "}\n",
    "\n",
    "print(\"âœ… ä¼˜åŒ–å™¨å‡½æ•°å·²å®šä¹‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ef84fa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# å•å…ƒæ ¼ 6: è¿è¡Œä¸‰æ•°æ®é›†å¯¹æ¯”å®éªŒ\n",
    "# ========================================================\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# é¢„ç®—è®¾ç½®ï¼ˆç¡®ä¿ >= 20ï¼‰\n",
    "budget = 20          # ä» 10 æ”¹ä¸º 20ï¼Œç¡®ä¿æ‰€æœ‰ä¼˜åŒ–å™¨æ­£å¸¸è¿è¡Œ\n",
    "n_runs = 3           # æµ‹è¯•é‡å¤æ¬¡æ•°ï¼ˆè°ƒé€šåæ”¹ä¸º 30ï¼‰\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for dataset_name in DATASET_FILES.keys():\n",
    "    print(f\"\\n{'='*50}\\nå¤„ç† {dataset_name}\\n{'='*50}\")\n",
    "    X, y = load_dataset(dataset_name)\n",
    "    print(f\"æ•°æ®å½¢çŠ¶: X={X.shape}, y={y.shape}\")\n",
    "    \n",
    "    def dataset_eval_func(params):\n",
    "        return evaluate_gbdt_for_dataset(X, y, params)\n",
    "    \n",
    "    print(\"å•çº¿ç¨‹æµ‹è¯•...\")\n",
    "    test_result = run_single_experiment(\"Random\", dataset_eval_func, 0, budget)\n",
    "    print(f\"æµ‹è¯•æˆåŠŸ: {test_result}\")\n",
    "    \n",
    "    print(f\"\\nå¹¶è¡Œå®éªŒ: {n_runs} æ¬¡ Ã— {len(optimizers)} ä¸ªä¼˜åŒ–å™¨\")\n",
    "    results = Parallel(n_jobs=-1, verbose=10)(\n",
    "        delayed(run_single_experiment)(opt_name, dataset_eval_func, run_id, budget)\n",
    "        for opt_name in optimizers.keys()\n",
    "        for run_id in range(n_runs)\n",
    "    )\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df[\"dataset\"] = dataset_name\n",
    "    all_results[dataset_name] = results_df\n",
    "    \n",
    "    output_dir = Path(\"./results\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    results_df.to_csv(output_dir / f\"{dataset_name}_results.csv\", index=False)\n",
    "    print(f\"\\n{dataset_name} å®Œæˆ!\")\n",
    "    print(results_df.groupby(\"optimizer\")[\"f1_score\"].agg([\"mean\", \"std\"]).round(4))\n",
    "\n",
    "print(\"\\nğŸ‰ æ‰€æœ‰æ•°æ®é›†å®Œæˆ!\")\n",
    "combined_results = pd.concat(all_results.values(), ignore_index=True)\n",
    "combined_results.to_csv(output_dir / \"all_datasets_results.csv\", index=False)\n",
    "print(f\"åˆå¹¶ç»“æœå·²ä¿å­˜ï¼Œæ€»è¡Œæ•°: {len(combined_results):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d197d79f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# åœ¨å•å…ƒæ ¼ 7 ä¸­ï¼Œç»˜åˆ¶å‰è¿‡æ»¤æ‰ NaN\n",
    "clean_results = results_df.dropna(subset=['f1_score'])\n",
    "pivot_df = clean_results.pivot(index=\"run_id\", columns=\"optimizer\", values=\"f1_score\")\n",
    "# è¿™æ · CD å›¾ä¼šè‡ªåŠ¨æ’é™¤ PSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d881bab9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# åœ¨å•å…ƒæ ¼ 7 æœ€åæ·»åŠ éªŒè¯ä»£ç \n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"å®éªŒå®Œæ•´æ€§éªŒè¯\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. æ£€æŸ¥æ˜¯å¦å­˜åœ¨ NaN\n",
    "nan_count = combined_results['f1_score'].isna().sum()\n",
    "print(f\"NaN å€¼æ•°é‡: {nan_count}ï¼ˆä¸»è¦æ¥è‡ª PSOï¼‰\")\n",
    "\n",
    "# 2. æ£€æŸ¥æ¯ä¸ªæ•°æ®é›†çš„ç»“æœ\n",
    "for dataset in all_results.keys():\n",
    "    df = all_results[dataset]\n",
    "    print(f\"\\n{dataset}:\")\n",
    "    print(f\"  - æ€»è¡Œæ•°: {len(df)}\")\n",
    "    print(f\"  - æœ‰æ•ˆç»“æœ: {df['f1_score'].notna().sum()}\")\n",
    "    print(f\"  - å¹³å‡ F1: {df['f1_score'].mean():.4f}\")\n",
    "\n",
    "# 3. ç¡®è®¤ CD å›¾æ–‡ä»¶å·²ç”Ÿæˆ\n",
    "figures_dir = Path(\"./figures\")\n",
    "cd_files = list(figures_dir.glob(\"cd_diagram_*.png\"))\n",
    "print(f\"\\nCD å›¾æ–‡ä»¶: {len(cd_files)} ä¸ª\")\n",
    "for f in cd_files:\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "print(\"\\nâœ… å®éªŒæˆåŠŸå®Œæˆï¼PSO çš„ NaN å·²è¢«è‡ªåŠ¨å¤„ç†ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d9d122",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# å•å…ƒæ ¼ 7: ç”Ÿæˆ LaTeX è¡¨æ ¼ + æ‰‹åŠ¨ CD å›¾ï¼ˆæ—  scikit_posthocs ä¾èµ–ï¼‰\n",
    "# ========================================================\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import friedmanchisquare\n",
    "from itertools import combinations\n",
    "\n",
    "figures_dir = Path(\"./figures\")\n",
    "figures_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# å­˜å‚¨æ‰€æœ‰ LaTeX è¡¨æ ¼\n",
    "latex_tables = {}\n",
    "\n",
    "def compute_critical_difference(k, n, alpha=0.05):\n",
    "    \"\"\"è®¡ç®— Nemenyi æ£€éªŒçš„ä¸´ç•Œå·®å€¼ CD\"\"\"\n",
    "    from scipy.stats import distributions\n",
    "    \n",
    "    # Nemenyi æ£€éªŒçš„ q å€¼è¡¨ï¼ˆè¿‘ä¼¼å…¬å¼ï¼Œé€‚ç”¨äº alpha=0.05ï¼‰\n",
    "    # q å€¼ç”¨äºå¤šç»„æ¯”è¾ƒ\n",
    "    if alpha == 0.05:\n",
    "        from scipy.stats import norm\n",
    "        # Nemenyi æ£€éªŒçš„ä¸´ç•Œå€¼è¿‘ä¼¼\n",
    "        # CD = q_alpha * sqrt(k*(k+1)/(6*n))\n",
    "        # å¯¹äº alpha=0.05, k=6, q â‰ˆ 2.94\n",
    "        # æ›´ç²¾ç¡®çš„è®¡ç®—ä½¿ç”¨ studentized range distribution\n",
    "        q_alpha = 2.94  # è¿‘ä¼¼å€¼ï¼Œé€‚ç”¨äº k=6\n",
    "    else:\n",
    "        q_alpha = norm.ppf(1 - alpha)\n",
    "    \n",
    "    CD = q_alpha * np.sqrt(k * (k + 1) / (6 * n))\n",
    "    return CD\n",
    "\n",
    "def plot_critical_difference_diagram(data, labels, alpha=0.05, title=\"\"):\n",
    "    \"\"\"æ‰‹åŠ¨ç»˜åˆ¶ CD å›¾\"\"\"\n",
    "    # data: DataFrameï¼Œæ¯åˆ—æ˜¯ä¸€ä¸ªç®—æ³•ï¼Œæ¯è¡Œæ˜¯ä¸€æ¬¡é‡å¤å®éªŒ\n",
    "    \n",
    "    # 1. è®¡ç®—æ¯ä¸ªç®—æ³•çš„å¹³å‡æ’å\n",
    "    # å¯¹æ¯ä¸ªå®éªŒï¼ˆè¡Œï¼‰ï¼Œè®¡ç®—æ’åï¼ˆ1=æœ€å¥½ï¼‰\n",
    "    rankings = data.rank(axis=1, ascending=False)  # False è¡¨ç¤ºå€¼è¶Šå¤§æ’åè¶Šå¥½\n",
    "    avg_ranks = rankings.mean().sort_values()\n",
    "    \n",
    "    # 2. è®¡ç®— CD\n",
    "    k = len(avg_ranks)  # ç®—æ³•æ•°é‡\n",
    "    n = len(data)       # é‡å¤å®éªŒæ¬¡æ•°\n",
    "    CD = compute_critical_difference(k, n, alpha)\n",
    "    \n",
    "    # 3. ç»˜åˆ¶\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # ç»˜åˆ¶ç®—æ³•æ’åçº¿\n",
    "    y_positions = np.arange(len(avg_ranks))\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(avg_ranks)))\n",
    "    \n",
    "    for i, (algo, rank) in enumerate(avg_ranks.items()):\n",
    "        # ç»˜åˆ¶æ°´å¹³çº¿ï¼ˆæ’åè½´ï¼‰\n",
    "        ax.plot([rank - 0.3, rank + 0.3], [i, i], color=colors[i], linewidth=4, solid_capstyle='butt')\n",
    "        # æ·»åŠ ç®—æ³•åç§°\n",
    "        ax.text(rank + 0.35, i, algo, va='center', ha='left', fontsize=11)\n",
    "    \n",
    "    # æ·»åŠ  CD çº¿\n",
    "    ax.plot([avg_ranks.iloc[0] - CD/2, avg_ranks.iloc[0] + CD/2], \n",
    "            [-0.5, -0.5], color='red', linewidth=2, label=f'CD Î±={alpha}')\n",
    "    ax.text(avg_ranks.iloc[0], -0.8, f'CD={CD:.3f}', ha='center', fontsize=10, color='red')\n",
    "    \n",
    "    # è®¾ç½®è½´\n",
    "    ax.set_xlim(avg_ranks.min() - 0.5, avg_ranks.max() + 1.5)\n",
    "    ax.set_ylim(-1.5, len(avg_ranks))\n",
    "    ax.set_xlabel('Average Rank', fontsize=12)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(title, fontsize=14, pad=20)\n",
    "    ax.legend(loc='upper right')\n",
    "    \n",
    "    # æ·»åŠ ç½‘æ ¼çº¿\n",
    "    ax.grid(True, axis='x', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "# ä¸ºæ¯ä¸ªæ•°æ®é›†ç”Ÿæˆå›¾è¡¨å’Œè¡¨æ ¼\n",
    "for dataset_name, results_df in all_results.items():\n",
    "    print(f\"\\nç”Ÿæˆ {dataset_name} çš„ CD å›¾...\")\n",
    "    \n",
    "    # æ¸…ç† NaN å€¼\n",
    "    clean_df = results_df.dropna(subset=['f1_score'])\n",
    "    \n",
    "    # å‡†å¤‡æ•°æ®\n",
    "    pivot_df = clean_df.pivot(index=\"run_id\", columns=\"optimizer\", values=\"f1_score\")\n",
    "    \n",
    "    # Friedman æ£€éªŒ\n",
    "    try:\n",
    "        stat, p_value = friedmanchisquare(*[pivot_df[col].values for col in pivot_df.columns])\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Friedman æ£€éªŒå¤±è´¥: {e}\")\n",
    "        stat, p_value = np.nan, np.nan\n",
    "    \n",
    "    # ç”Ÿæˆ CD å›¾\n",
    "    fig, ax = plot_critical_difference_diagram(\n",
    "        pivot_df, \n",
    "        pivot_df.columns, \n",
    "        alpha=0.05,\n",
    "        title=f\"Critical Difference Diagram â€“ {dataset_name}\"\n",
    "    )\n",
    "    \n",
    "    plt.savefig(figures_dir / f\"cd_diagram_{dataset_name.lower()}.png\", dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # ç”Ÿæˆ LaTeX è¡¨æ ¼\n",
    "    stats = clean_df.groupby(\"optimizer\")[\"f1_score\"].agg([\"mean\", \"std\"]).round(4)\n",
    "    stats.columns = [\"Mean F1\", \"Std Dev\"]\n",
    "    \n",
    "    p_text = f\"Friedman p={p_value:.4e}\" if not np.isnan(p_value) else \"Friedman test failed\"\n",
    "    latex_table = stats.to_latex(\n",
    "        caption=f\"Performance on {dataset_name} Dataset ({p_text})\",\n",
    "        label=f\"tab:{dataset_name.lower()}_results\",\n",
    "        position=\"htbp\",\n",
    "        column_format=\"lcc\"\n",
    "    )\n",
    "    \n",
    "    # æœ€ä½³ç»“æœåŠ ç²—\n",
    "    best_mean = stats[\"Mean F1\"].max()\n",
    "    latex_table = latex_table.replace(f\"{best_mean:.4f}\", f\"\\\\textbf{{{best_mean:.4f}}}\")\n",
    "    \n",
    "    latex_tables[dataset_name] = latex_table\n",
    "\n",
    "# è·¨æ•°æ®é›†æ±‡æ€»è¡¨æ ¼\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ç”Ÿæˆè·¨æ•°æ®é›†æ±‡æ€»è¡¨æ ¼...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary_stats = combined_results.groupby([\"dataset\", \"optimizer\"])[\"f1_score\"].agg([\"mean\", \"std\"]).round(4)\n",
    "summary_stats.columns = [\"Mean F1\", \"Std Dev\"]\n",
    "wide_table = summary_stats[\"Mean F1\"].unstack(level=0)\n",
    "\n",
    "latex_summary = wide_table.to_latex(\n",
    "    caption=\"Cross-Dataset Performance Summary (Mean $\\pm$ Std)\",\n",
    "    label=\"tab:cross_dataset_summary\",\n",
    "    position=\"htbp\",\n",
    "    column_format=\"lccc\"\n",
    ")\n",
    "\n",
    "for col in wide_table.columns:\n",
    "    best_val = wide_table[col].max()\n",
    "    latex_summary = latex_summary.replace(f\"{best_val:.4f}\", f\"\\\\textbf{{{best_val:.4f}}}\")\n",
    "\n",
    "latex_tables[\"Cross_Dataset_Summary\"] = latex_summary\n",
    "\n",
    "# ä¿å­˜ LaTeX æ–‡ä»¶\n",
    "tables_dir = Path(\"./tables\")\n",
    "tables_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for name, table in latex_tables.items():\n",
    "    (tables_dir / f\"{name}_table.tex\").write_text(table, encoding='utf-8')\n",
    "    print(f\"âœ… è¡¨æ ¼ {name}.tex å·²ä¿å­˜\")\n",
    "\n",
    "# æ‰“å°é¢„è§ˆ\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LaTeX è¡¨æ ¼é¢„è§ˆ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, table in latex_tables.items():\n",
    "    print(f\"\\n{'='*20} {name} {'='*20}\")\n",
    "    print(table)\n",
    "\n",
    "print(\"\\nğŸ‰ æ‰€æœ‰ LaTeX è¡¨æ ¼å’Œ CD å›¾ç”Ÿæˆå®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf7fbc7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006af60b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
